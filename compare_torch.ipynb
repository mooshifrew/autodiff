{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from autodiff.activation import ReLU, Linear\n",
    "from autodiff.network import Network, NetworkParams\n",
    "\n",
    "file_path = 'test-parameters.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as file:\n",
    "    data = pkl.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_def: NetworkParams = {\n",
    "    \"input_shape\": 2,\n",
    "    \"output_shape\": 1,\n",
    "    \"layers\": [\n",
    "        {\n",
    "            \"input_shape\": 2,\n",
    "            \"n_neurons\": len(data['w1']),\n",
    "            \"weight_init\": data['w1'],\n",
    "            \"bias_init\": data['b1'] ,\n",
    "            \"activation\": ReLU(),\n",
    "        },\n",
    "        {\n",
    "            \"input_shape\": len(data['w1']),\n",
    "            \"n_neurons\": len(data['w2']),\n",
    "            \"weight_init\": data['w2'],\n",
    "            \"bias_init\": data['b2'] ,\n",
    "            \"activation\": ReLU(),\n",
    "        },\n",
    "        {\n",
    "            \"input_shape\": len(data['w2']),\n",
    "            \"n_neurons\": len(data['w3']),\n",
    "            \"weight_init\": data['w3'],\n",
    "            \"bias_init\": data['b3'] ,\n",
    "            \"activation\": Linear(), \n",
    "        }\n",
    "    ]\n",
    "} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Backprop Grads\n",
      "####################\n",
      "First Layer w_grads:    [[-0.18804252 -0.28206378]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.53346761  0.80020142]\n",
      " [ 0.14224417  0.21336625]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [-0.02312626 -0.0346894 ]\n",
      " [ 0.          0.        ]]\n",
      "First Layer b_grads:    [-0.09402126  0.          0.          0.26673381  0.07112208  0.\n",
      "  0.          0.         -0.01156313  0.        ]\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from model import SimpleNet, set_model_weights\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_data = [2,3]\n",
    "target_data = [1]\n",
    "\n",
    "def get_loss(y, y_hat): \n",
    "    return 0.5*((y_hat - y)**2)\n",
    "\n",
    "# custom network\n",
    "network = Network(network_def)\n",
    "y_pred = network.forward(input_data)\n",
    "y = target_data\n",
    "loss = get_loss(y, y_pred)\n",
    "network.backward(y_pred - y) \n",
    "print(\"Custom Backprop Grads\")\n",
    "print(\"#\"*20)\n",
    "print(\"First Layer w_grads:   \", network.layers[0].w_grads)\n",
    "print(\"First Layer b_grads:   \", network.layers[0].b_grads)\n",
    "print(\"#\"*20)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "input_data = torch.tensor([2,3], dtype=torch.float64)\n",
    "target_data = torch.tensor([1], dtype=torch.float64)\n",
    "\n",
    "# pytorch network\n",
    "model = SimpleNet()\n",
    "set_model_weights(data,model)\n",
    "output = model(input_data)\n",
    "loss = criterion(output, target_data)/2\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1_weight_grad: tensor([[-0.1880, -0.2821],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.5335,  0.8002],\n",
      "        [ 0.1422,  0.2134],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [-0.0231, -0.0347],\n",
      "        [ 0.0000,  0.0000]], dtype=torch.float64)\n",
      "fc1_bias_grad: tensor([-0.0940,  0.0000,  0.0000,  0.2667,  0.0711,  0.0000,  0.0000,  0.0000,\n",
      "        -0.0116,  0.0000], dtype=torch.float64)\n",
      "{'fc1_weight_grad': tensor([[-0.1880, -0.2821],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.5335,  0.8002],\n",
      "        [ 0.1422,  0.2134],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [-0.0231, -0.0347],\n",
      "        [ 0.0000,  0.0000]], dtype=torch.float64), 'fc2_weight_grad': tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0535, 0.0000, 0.0000, 0.0844, 0.0124, 0.0000, 0.0000, 0.0000, 0.0536,\n",
      "         0.0000],\n",
      "        [0.6646, 0.0000, 0.0000, 1.0482, 0.1545, 0.0000, 0.0000, 0.0000, 0.6658,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0048, 0.0000, 0.0000, 0.0076, 0.0011, 0.0000, 0.0000, 0.0000, 0.0048,\n",
      "         0.0000],\n",
      "        [0.3677, 0.0000, 0.0000, 0.5800, 0.0855, 0.0000, 0.0000, 0.0000, 0.3684,\n",
      "         0.0000],\n",
      "        [0.5669, 0.0000, 0.0000, 0.8942, 0.1318, 0.0000, 0.0000, 0.0000, 0.5680,\n",
      "         0.0000]], dtype=torch.float64), 'fc3_weight_grad': tensor([[ 0.0000,  0.0000,  0.0000, -1.2141, -0.9971,  0.0000,  0.0000, -0.1515,\n",
      "         -1.0900, -0.7580]], dtype=torch.float64), 'fc1_bias_grad': tensor([-0.0940,  0.0000,  0.0000,  0.2667,  0.0711,  0.0000,  0.0000,  0.0000,\n",
      "        -0.0116,  0.0000], dtype=torch.float64), 'fc2_bias_grad': tensor([0.0000, 0.0000, 0.0000, 0.0387, 0.4801, 0.0000, 0.0000, 0.0035, 0.2657,\n",
      "        0.4096], dtype=torch.float64), 'fc3_bias_grad': tensor([-1.6902], dtype=torch.float64)}\n",
      "fc1_weight_grad: tensor([[-0.1880, -0.2821],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.5335,  0.8002],\n",
      "        [ 0.1422,  0.2134],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [-0.0231, -0.0347],\n",
      "        [ 0.0000,  0.0000]], dtype=torch.float64)\n",
      "fc1_bias_grad: tensor([-0.0940,  0.0000,  0.0000,  0.2667,  0.0711,  0.0000,  0.0000,  0.0000,\n",
      "        -0.0116,  0.0000], dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mget_gradients())\n\u001b[1;32m      4\u001b[0m weight_gradients \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_gradients()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfc1_weight_grad\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m pytorch_w_gradients \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Custom backpropagation gradients obtained from a forward pass\u001b[39;00m\n\u001b[1;32m      8\u001b[0m custom_w_gradients \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.18804252\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.28206378\u001b[39m],\n\u001b[1;32m      9\u001b[0m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     10\u001b[0m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.02312626\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.0346894\u001b[39m ],\n\u001b[1;32m     17\u001b[0m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.0/lib/python3.9/site-packages/torch/_tensor.py:1062\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1062\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "# PyTorch gradients obtained from a forward pass\n",
    "print(model.get_gradients())\n",
    "pytorch_w_gradients = model.get_gradients()['fc1_weight_grad'].detach().cpu().numpy()\n",
    "\n",
    "# Custom backpropagation gradients obtained from a forward pass\n",
    "custom_w_gradients = np.array([[-0.18804252, -0.28206378],\n",
    "[0, 0],\n",
    "[0, 0],\n",
    "[0.53346761, 0.80020142],\n",
    "[0.14224417, 0.21336625],\n",
    "[0, 0],\n",
    "[0, 0],\n",
    "[0, 0],\n",
    "[-0.02312626, -0.0346894 ],\n",
    "[0, 0]])\n",
    "\n",
    "\n",
    "# Returns false if the gradients do not match, otherwise returns true if the gradients match within the given tolerance\n",
    "def compare_gradients(pytorch_gradients: np.ndarray, custom_backprop_gradients: np.ndarray, tolerance: float) -> bool:\n",
    "    if pytorch_gradients.shape != custom_backprop_gradients.shape:\n",
    "        raise ValueError('The shapes do not match') # Early stopping if the gradients to compare do not have compatible shapes\n",
    "\n",
    "    return np.allclose(pytorch_gradients, custom_backprop_gradients, atol=tolerance)\n",
    "\n",
    "compare_gradients(pytorch_gradients=pytorch_w_gradients, custom_backprop_gradients=custom_w_gradients, tolerance=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
