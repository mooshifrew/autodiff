{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from autodiff.activation import ReLU, Linear\n",
    "from autodiff.network import Network, NetworkParams\n",
    "\n",
    "file_path = 'test-parameters.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as file:\n",
    "    data = pkl.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_def: NetworkParams = {\n",
    "    \"input_shape\": 2,\n",
    "    \"output_shape\": 1,\n",
    "    \"layers\": [\n",
    "        {\n",
    "            \"input_shape\": 2,\n",
    "            \"n_neurons\": len(data['w1']),\n",
    "            \"weight_init\": data['w1'],\n",
    "            \"bias_init\": data['b1'] ,\n",
    "            \"activation\": ReLU(),\n",
    "        },\n",
    "        {\n",
    "            \"input_shape\": len(data['w1']),\n",
    "            \"n_neurons\": len(data['w2']),\n",
    "            \"weight_init\": data['w2'],\n",
    "            \"bias_init\": data['b2'] ,\n",
    "            \"activation\": ReLU(),\n",
    "        },\n",
    "        {\n",
    "            \"input_shape\": len(data['w2']),\n",
    "            \"n_neurons\": len(data['w3']),\n",
    "            \"weight_init\": data['w3'],\n",
    "            \"bias_init\": data['b3'] ,\n",
    "            \"activation\": Linear(), \n",
    "        }\n",
    "    ]\n",
    "} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data=array([0.09762701, 0.43037873]),  1.0738026310047295\n",
      "Custom Backprop Grads\n",
      "####################\n",
      "First Layer w_grads:    [[-0.0210035  -0.09259178]\n",
      " [-0.0184767  -0.08145263]\n",
      " [ 0.          0.        ]\n",
      " [ 0.01776593  0.0783193 ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [-0.00974717 -0.04296942]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]]\n",
      "First Layer b_grads:    [-0.21514022 -0.18925803  0.          0.18197762  0.          0.\n",
      " -0.09984094  0.          0.          0.        ]\n",
      "####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\anaconda3\\envs\\syde577\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:538: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from model import SimpleNet, set_model_weights\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_data = data['inputs'][0]\n",
    "target_data = data['targets'][0]\n",
    "print(f\"{input_data=},  {target_data}\")\n",
    "\n",
    "def get_loss(y, y_hat): \n",
    "    return 0.5*((y_hat - y)**2)\n",
    "\n",
    "# custom network\n",
    "network = Network(network_def)\n",
    "y_pred = network.forward(input_data)\n",
    "y = target_data\n",
    "loss = get_loss(y, y_pred)\n",
    "network.backward(y_pred - y) \n",
    "print(\"Custom Backprop Grads\")\n",
    "print(\"#\"*20)\n",
    "custom_w_gradients = network.layers[0].w_grads\n",
    "custom_b_gradients = network.layers[0].b_grads\n",
    "print(\"First Layer w_grads:   \", custom_w_gradients)\n",
    "print(\"First Layer b_grads:   \", custom_b_gradients)\n",
    "print(\"#\"*20)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "input_data = torch.tensor(input_data, dtype=torch.float64)\n",
    "target_data = torch.tensor(target_data, dtype=torch.float64)\n",
    "\n",
    "# pytorch network\n",
    "model = SimpleNet()\n",
    "set_model_weights(data,model)\n",
    "output = model(input_data)\n",
    "loss = criterion(output, target_data)/2\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1_weight_grad: tensor([[-0.0210, -0.0926],\n",
      "        [-0.0185, -0.0815],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0178,  0.0783],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [-0.0097, -0.0430],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000]], dtype=torch.float64)\n",
      "fc1_bias_grad: tensor([-0.2151, -0.1893,  0.0000,  0.1820,  0.0000,  0.0000, -0.0998,  0.0000,\n",
      "         0.0000,  0.0000], dtype=torch.float64)\n",
      "fc1_weight_grad: tensor([[-0.0210, -0.0926],\n",
      "        [-0.0185, -0.0815],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0178,  0.0783],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [-0.0097, -0.0430],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000]], dtype=torch.float64)\n",
      "fc1_bias_grad: tensor([-0.2151, -0.1893,  0.0000,  0.1820,  0.0000,  0.0000, -0.0998,  0.0000,\n",
      "         0.0000,  0.0000], dtype=torch.float64)\n",
      "Gradients match W: True\n",
      "Gradients match B: True\n"
     ]
    }
   ],
   "source": [
    "# Extract the weight gradients (first element of the tuple)\n",
    "pytorch_w_gradients = model.get_gradients()[0].detach().cpu().numpy()\n",
    "pytorch_b_gradients = model.get_gradients()[1].detach().cpu().numpy()\n",
    "\n",
    "\n",
    "# Comparison function\n",
    "def compare_gradients(pytorch_gradients: np.ndarray, custom_backprop_gradients: np.ndarray, tolerance: float) -> bool:\n",
    "    if pytorch_gradients.shape != custom_backprop_gradients.shape:\n",
    "        raise ValueError('The shapes do not match')\n",
    "\n",
    "    return np.allclose(pytorch_gradients, custom_backprop_gradients, atol=tolerance)\n",
    "\n",
    "# Run the comparison\n",
    "result_w = compare_gradients(\n",
    "    pytorch_gradients=pytorch_w_gradients,\n",
    "    custom_backprop_gradients=custom_w_gradients,\n",
    "    tolerance=0.001\n",
    ")\n",
    "\n",
    "print(f\"Gradients match W: {result_w}\")\n",
    "\n",
    "result_b = compare_gradients(\n",
    "    pytorch_gradients=pytorch_b_gradients,\n",
    "    custom_backprop_gradients=custom_b_gradients,\n",
    "    tolerance=0.001\n",
    ")\n",
    "\n",
    "print(f\"Gradients match B: {result_b}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syde577",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
