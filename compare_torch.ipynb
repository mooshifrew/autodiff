{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from autodiff.activation import ReLU, Linear\n",
    "from autodiff.network import Network, NetworkParams\n",
    "\n",
    "file_path = 'test-parameters.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as file:\n",
    "    data = pkl.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_def: NetworkParams = {\n",
    "    \"input_shape\": 2,\n",
    "    \"output_shape\": 1,\n",
    "    \"layers\": [\n",
    "        {\n",
    "            \"input_shape\": 2,\n",
    "            \"n_neurons\": len(data['w1']),\n",
    "            \"weight_init\": data['w1'],\n",
    "            \"bias_init\": data['b1'] ,\n",
    "            \"activation\": ReLU(),\n",
    "        },\n",
    "        {\n",
    "            \"input_shape\": len(data['w1']),\n",
    "            \"n_neurons\": len(data['w2']),\n",
    "            \"weight_init\": data['w2'],\n",
    "            \"bias_init\": data['b2'] ,\n",
    "            \"activation\": ReLU(),\n",
    "        },\n",
    "        {\n",
    "            \"input_shape\": len(data['w2']),\n",
    "            \"n_neurons\": len(data['w3']),\n",
    "            \"weight_init\": data['w3'],\n",
    "            \"bias_init\": data['b3'] ,\n",
    "            \"activation\": Linear(), \n",
    "        }\n",
    "    ]\n",
    "} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Backprop Grads\n",
      "####################\n",
      "First Layer w_grads:    [[-0.18804252 -0.28206378]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.53346761  0.80020142]\n",
      " [ 0.14224417  0.21336625]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [-0.02312626 -0.0346894 ]\n",
      " [ 0.          0.        ]]\n",
      "First Layer b_grads:    [-0.09402126  0.          0.          0.26673381  0.07112208  0.\n",
      "  0.          0.         -0.01156313  0.        ]\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from model import SimpleNet, set_model_weights\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_data = [2,3]\n",
    "target_data = [1]\n",
    "\n",
    "def get_loss(y, y_hat): \n",
    "    return 0.5*((y_hat - y)**2)\n",
    "\n",
    "# custom network\n",
    "network = Network(network_def)\n",
    "y_pred = network.forward(input_data)\n",
    "y = target_data\n",
    "loss = get_loss(y, y_pred)\n",
    "network.backward(y_pred - y) \n",
    "print(\"Custom Backprop Grads\")\n",
    "print(\"#\"*20)\n",
    "print(\"First Layer w_grads:   \", network.layers[0].w_grads)\n",
    "print(\"First Layer b_grads:   \", network.layers[0].b_grads)\n",
    "print(\"#\"*20)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "input_data = torch.tensor([2,3], dtype=torch.float64)\n",
    "target_data = torch.tensor([1], dtype=torch.float64)\n",
    "\n",
    "# pytorch network\n",
    "model = SimpleNet()\n",
    "set_model_weights(data,model)\n",
    "output = model(input_data)\n",
    "loss = criterion(output, target_data)/2\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1_weight_grad: tensor([[-0.1880, -0.2821],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.5335,  0.8002],\n",
      "        [ 0.1422,  0.2134],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [-0.0231, -0.0347],\n",
      "        [ 0.0000,  0.0000]], dtype=torch.float64)\n",
      "fc1_bias_grad: tensor([-0.0940,  0.0000,  0.0000,  0.2667,  0.0711,  0.0000,  0.0000,  0.0000,\n",
      "        -0.0116,  0.0000], dtype=torch.float64)\n",
      "fc1_weight_grad: tensor([[-0.1880, -0.2821],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.5335,  0.8002],\n",
      "        [ 0.1422,  0.2134],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [-0.0231, -0.0347],\n",
      "        [ 0.0000,  0.0000]], dtype=torch.float64)\n",
      "fc1_bias_grad: tensor([-0.0940,  0.0000,  0.0000,  0.2667,  0.0711,  0.0000,  0.0000,  0.0000,\n",
      "        -0.0116,  0.0000], dtype=torch.float64)\n",
      "Gradients match W: True\n",
      "Gradients match B: True\n"
     ]
    }
   ],
   "source": [
    "# Extract the weight gradients (first element of the tuple)\n",
    "pytorch_w_gradients = model.get_gradients()[0].detach().cpu().numpy()\n",
    "pytorch_b_gradients = model.get_gradients()[1].detach().cpu().numpy()\n",
    "\n",
    "# Custom backpropagation gradients (already given) as printed above\n",
    "custom_w_gradients = np.array([\n",
    "    [-0.18804252, -0.28206378],\n",
    "    [0, 0],\n",
    "    [0, 0],\n",
    "    [0.53346761, 0.80020142],\n",
    "    [0.14224417, 0.21336625],\n",
    "    [0, 0],\n",
    "    [0, 0],\n",
    "    [0, 0],\n",
    "    [-0.02312626, -0.0346894],\n",
    "    [0, 0]\n",
    "])\n",
    "custom_b_gradients = np.array([-0.09402126, 0, 0, 0.26673381, 0.07112208, 0, 0, 0, -0.01156313, 0])\n",
    "\n",
    "\n",
    "# Comparison function\n",
    "def compare_gradients(pytorch_gradients: np.ndarray, custom_backprop_gradients: np.ndarray, tolerance: float) -> bool:\n",
    "    if pytorch_gradients.shape != custom_backprop_gradients.shape:\n",
    "        raise ValueError('The shapes do not match')\n",
    "\n",
    "    return np.allclose(pytorch_gradients, custom_backprop_gradients, atol=tolerance)\n",
    "\n",
    "# Run the comparison\n",
    "result_w = compare_gradients(\n",
    "    pytorch_gradients=pytorch_w_gradients,\n",
    "    custom_backprop_gradients=custom_w_gradients,\n",
    "    tolerance=0.001\n",
    ")\n",
    "\n",
    "print(f\"Gradients match W: {result_w}\")\n",
    "\n",
    "result_b = compare_gradients(\n",
    "    pytorch_gradients=pytorch_b_gradients,\n",
    "    custom_backprop_gradients=custom_b_gradients,\n",
    "    tolerance=0.001\n",
    ")\n",
    "\n",
    "print(f\"Gradients match B: {result_b}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
